{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0edeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f9e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee7f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "ec3ebe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объем выборки для каждой группы в сплит-тесте: 3014\n",
      "CTR в контрольной группе: 0.02339572192513369\n",
      "CTR в тестовой группе: 0.046984572230014024\n",
      "Z-статистика: -3.472544338409275\n",
      "P-значение: 0.0005155498455473644\n",
      "Различие в CTR статистически значимо\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Дима\\AppData\\Local\\Temp\\ipykernel_8584\\718430631.py:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df0 = pd.concat([df0, users_df0], ignore_index=True)\n",
      "C:\\Users\\Дима\\AppData\\Local\\Temp\\ipykernel_8584\\718430631.py:86: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df1 = pd.concat([df1, users_df1], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Используем <<Заданные параметры>> для определения объема выборки для сплита 50/50 и запуска эксперимента.\n",
    "# Эксперимент рассчитан для идеальных условий.\n",
    "# Можно \"Поиграться\" в <<Заданных параметрах>> и менять значения\n",
    "\n",
    "# \"О чём эксперимент?\" \n",
    "# Мы моделируем эксперимент в организации. \n",
    "# У нас есть объявления, приходит продакт и говорит: \n",
    "# \"Есть новая фича, необходимо проанализировать его влияние на CTR, прогнозируем увеличение CTR на 2%\"\n",
    "# Мы говорим \"Окей\" и моделируем эксперимент:\n",
    "#     1) Определяем объем выборки, который сможет задетектировать MDE по \"Заданным параметрам\";\n",
    "#     2) Определяем, какой у нас объем трафика;\n",
    "#     3) Делим трафик на две группы;\n",
    "#     4) В первой группе, у нас базовый CTR;\n",
    "#     5) Во второй группе определяем возможность увеличения CTR на 2%;\n",
    "#     6) Расчитываем CTR;\n",
    "#     7) Проводим тест;\n",
    "# Для надежности эксперимента мы перемешиваем рандомно основной входящий трафик, \n",
    "# после чего делим его для тестовой и контрольной группы.\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "# <<Заданные параметры>>\n",
    "base_ctr = 0.03   # базовый CTR\n",
    "MDE = 0.02        # минимальный эффект который мы хотим обнаружить\n",
    "alpha = 0.05      # уровень значимости\n",
    "n = 0.8           # мощность теста\n",
    "m = 5000          # количество пользователей в день\n",
    "t = m/2           # делим трафик по группам\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "# Расчет стандартного отклонения для Z-теста на равенство долей\n",
    "pooled_prob = (base_ctr + base_ctr + MDE) / 2\n",
    "std_dev = math.sqrt(2 * pooled_prob * (1 - pooled_prob))\n",
    "\n",
    "# Расчет Z-значения для уровня значимости alpha\n",
    "Z_alpha = norm.ppf(1 - alpha/2)\n",
    "\n",
    "# Расчет объема выборки для каждой группы\n",
    "n = (2 * (std_dev * Z_alpha + std_dev * norm.ppf(n))**2) / MDE**2\n",
    "n = math.ceil(n)\n",
    "\n",
    "print(\"Объем выборки для каждой группы в сплит-тесте:\", n)\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "# Генерация идентификаторов пользователей\n",
    "user_ids = ['{:011d}'.format(i) for i in range(1, n*2+1)]\n",
    "\n",
    "# Генерация дат для пользователей, зависит от трафика и объема группы\n",
    "dates = []\n",
    "for i in range(n*2):\n",
    "    dates.append(pd.Timestamp('2022-01-01') + pd.DateOffset(days=i // m))\n",
    "\n",
    "# Создание датафрейма df\n",
    "df = pd.DataFrame({'userid': user_ids, 'show': 0, 'click': 0, 'dt': dates})\n",
    "\n",
    "# Создание пустых датафреймов df0 и df1\n",
    "df0 = pd.DataFrame(columns=['userid', 'show', 'click', 'dt'])\n",
    "df1 = pd.DataFrame(columns=['userid', 'show', 'click', 'dt'])\n",
    "\n",
    "    \n",
    "for date in df['dt'].unique():\n",
    "    \n",
    "    #перемешиваем данные рандомно за дни последовательно в цикле\n",
    "    users_day = df[df['dt'] == date]['userid'].unique()\n",
    "    np.random.shuffle(users_day)\n",
    "    \n",
    "    #делим перемешанные данные по группам \n",
    "    sampls = len(users_day) // 2\n",
    "    users0 = users_day[:sampls]\n",
    "    users1 = users_day[sampls:]\n",
    "    \n",
    "    # Отбор строк из фрейма данных по значениям в группы \n",
    "    users_df0 = df[df['userid'].isin(users0)]\n",
    "    users_df1 = df[df['userid'].isin(users1)]\n",
    "    \n",
    "    # добавление данных за день в группы\n",
    "    df0 = pd.concat([df0, users_df0], ignore_index=True)\n",
    "    df1 = pd.concat([df1, users_df1], ignore_index=True)\n",
    "\n",
    "# Генерируем маркеры показа и клика для контрольной и тестовой групп\n",
    "# В контрольной группе оставляем базовый мараметр CTR, так как он обычно нам известен\n",
    "# В тестовую группу добавляем эффект который мы хотим обнаружить\n",
    "df0['show'] = np.random.choice([0, 1], size=len(df0), p=[1-t/m, t/m])\n",
    "df0['click'] = np.where((df0['show'] == 1) & (np.random.rand(len(df0)) < base_ctr), 1, 0)\n",
    "\n",
    "df1['show'] = np.random.choice([0, 1], size=len(df1), p=[1-t/m, t/m])\n",
    "df1['click'] = np.where((df1['show'] == 1) & (np.random.rand(len(df1)) < base_ctr + MDE), 1, 0)\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "# Рассчитываем CTR для контрольной и тестовой групп\n",
    "control_clicks = df0['click'].sum()\n",
    "control_shows = df0['show'].sum()\n",
    "control_ctr = control_clicks / control_shows\n",
    "\n",
    "test_clicks = df1['click'].sum()\n",
    "test_shows = df1['show'].sum()\n",
    "test_ctr = test_clicks / test_shows\n",
    "\n",
    "# Проводим статистический тест (например, z-тест)\n",
    "z_score, p_value = sm.stats.proportions_ztest([control_clicks, test_clicks], [control_shows, test_shows])\n",
    "\n",
    "# Вывод результатов\n",
    "print(f\"CTR в контрольной группе: {control_ctr}\")\n",
    "print(f\"CTR в тестовой группе: {test_ctr}\")\n",
    "print(f\"Z-статистика: {z_score}\")\n",
    "print(f\"P-значение: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Различие в CTR статистически значимо\")\n",
    "else:\n",
    "    print(\"Различие в CTR не является статистически значимым\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a215a7",
   "metadata": {},
   "source": [
    "Есть три товара по 10, 50 и 100 рублей. Было сделано 1000 покупок. Вы собрали описательные статистики:\n",
    "\n",
    "* Средний чек 9,8\n",
    "* Дисперсия 34.8\n",
    "* Мода 10\n",
    "* Самая большая покупка 70р\n",
    "Все ли с ними хорошо, поясните почему\n",
    "\n",
    "Есть две гипотезы, которые сразу можно определить, когда мы видим эти значения:\n",
    "- Гипотеза первая: товары \"цельные\", и тогда в полученных показателях закралась ошибка при подсчете этих данных. Почему мы так считаем и какие могут быть ошибки?\n",
    "\n",
    "Почему мы так считаем? Разберем каждый показатель по отдельности:\n",
    "\n",
    "1. Мода - 10, вопросов нет: есть товар дешевый за 10р, покупок больше, все логично!\n",
    "2. Самая большая покупка - 70р, что тоже возможно, при чеке: 10+10+50 и тд в разных вариациях.\n",
    "3. Средний чек - 9.8 такой показатель возможен, если у нас чек без товаров,но это маловероятно! - первый вопросик к данным!\n",
    "4. Дисперсия, 34.8 она возможна, если примем к сведению, то что средний чек нереалистичен в условиях данной гипотезы и его опустим.\n",
    "5. Есть товар в 100р, если принять к сведению то, что максимальный чек = 70р, то получается, что его вообще не покупали. Тогда вопрос, зачем выставляли этот товар! - второй вопросик к данным!\n",
    "\n",
    "Какие могут быть ошибки?:\n",
    "1. Возможна ошибка при загрузке источников - подтянулись не те данные и \"подтянулись\" данные из других источников!\n",
    "2. Возможно при процессе предобработки данных скрипт выполнил не те агрегации и данные сьехали!\n",
    "3. Возможно при джоине справочников(по метаданным) произошел сбой и подтянулись пустые чеки!\n",
    "4. Возможно при выгрузке аналитиком, он подгрузил их не из той таблице!\n",
    "\n",
    "Вариантов много, но вывод один: данные не согласуются с гипотезой и , если действительно товары 'цельные', необходимо более детально изучить данные и выяснить, что могло повлиять на такие результаты!\n",
    "\n",
    "Рассмотрим вторую гипотезу: данные \"Весовые\", то есть у нас есть товары за 10, 50, 100р - где ценна указанна за определенную долю веса, например кг. Рассмотрим в данном случае описательные статистики!:\n",
    "1. Мода -10, вопросов нет: есть товар дешевый за 10р, покупок больше, все логично!\n",
    "2. Самая большая покупка - 70р, такой показатель возможен набором различных товаров!\n",
    "3. Средний чек 9.8, такой показатель возможен, скорее всего тавары не частого применения(например специи) и нужды в больших количествах нет! например взяли все по 5 гр\n",
    "4. Дисперсия 34.8 возможна, стандартное отклонение: корень из 34.8 = 5.9 - скорее всего больше 90% всех наблюдений(общий чек) лежит: 0<90% наблюдений< 19.5 (5.9*1.64+9.8)\n",
    "Вывод по второй гипотезе: Товары с ценами 10, 50, 100 можно различным образом комбинировать в одном чеке, при этом если они весовые. Данные реальны при второй гипотезе.\n",
    "\n",
    "Таким образом, так как у нас нет точки отсчета, в каком виде у нас представленны товары, по которым собиралась статистика, я больше всего склоняюсь к тому что у нас верна гипотеза под номером два - товары весовые, такая описательная статистика возможна! \n",
    "Если, при анализе оказалось, что товары цельные, то данные нереальны: возможно ошибки при обработке данных, дубли или ошибки иного характера при получении данных из источника."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "c64c57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics \n",
    "# Создаем список чисел для расчета дисперсии\n",
    "data = [1, 10, 10,  70, 2, 4.6, 3, 6.8, 5, 1, 10, 10, 0.1]\n",
    "\n",
    "\n",
    "# Рассчитываем статистики\n",
    "m = np.mean(data)\n",
    "var = np.var(data)\n",
    "std = np.std(data)\n",
    "mad = statistics.mode(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "52d5de91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "ef7f8568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.26923076923077"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "faadaf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310.49751479289944"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8104a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "Есть три товара по 10, 50 и 100 рублей. Было сделано 1000 покупок. Вы собрали описательные статистики:\n",
    "\n",
    "* Средний чек 9,8\n",
    "* Дисперсия 34.8\n",
    "* Мода 10\n",
    "* Самая большая покупка 70р"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "d1ff51b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение: 59.068301084702995\n",
      "Дисперсия: 34.8\n",
      "Стандартное отклонение: 5.89915248150105\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Генерируем случайные данные\n",
    "np.random.seed(42)  # Задаем seed для воспроизводимости результатов\n",
    "observations = np.random.normal(0, 1, 1000)  # Генерируем данные с нулевым средним и единичной дисперсией\n",
    "\n",
    "# Корректируем данные, чтобы среднее значение было равно 9.8 и дисперсия равна 34.8\n",
    "observations = observations - np.mean(observations) + 9.8\n",
    "observations = observations * np.sqrt(34.8 / np.var(observations))\n",
    "\n",
    "# Рассчитываем статистики\n",
    "mean_observation = np.mean(observations)\n",
    "variance_observation = np.var(observations)\n",
    "std_deviation_observation = np.std(observations)\n",
    "\n",
    "print(\"Среднее значение:\", mean_observation)\n",
    "print(\"Дисперсия:\", variance_observation)\n",
    "print(\"Стандартное отклонение:\", std_deviation_observation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9d5261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fd01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9525db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c35e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
